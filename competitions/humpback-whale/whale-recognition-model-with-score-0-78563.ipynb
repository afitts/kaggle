{"cells":[{"metadata":{"_uuid":"baff2752922fc86a6d62d3c8e02dd5d868a9d296","collapsed":true},"cell_type":"markdown","source":"# Whale Classification Model\nThis notebook describes the strategy behind the 0.78563 submission to the Humpack Whale identification Challenge.\n\nIt should be studied in conjunction with the [Bounding Box Model](http://www.kaggle.com/martinpiotte/bounding-box-model) notebook which describes separately the strategy for image cropping.\n\nTo speed things up, the results of some slow computations are included as a dataset instead of being recomputed here. However, the code is still provided in the notebook as reference, even if it is not executed by default."},{"metadata":{"_uuid":"3c7ec1841dc33ba31d374ab9f45cab51ba5b9886"},"cell_type":"markdown","source":"# Abstract\nThe approach used for this submission is essentially a [Siamese Neural Network](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf), with a few modifications that will be covered in details later. The element that generated the largest accuracy improvement is the procedure used to generate image pairs during training. Each training epoch is composed of a sequence of image pairs (A, B) such that:\n\n* Exactly 50% of the pairs are for matching whales, and 50% for different whales;\n* Each image from the training set is used exactly 4 times per epoch: A and B images of matching whales, A and B images of different whale pairs;\n* Pairs of images of different whales are selected to be difficult for the network to distinguish at a given stage of the training. This is inspired from adversarial training: find pairs of images that are from different whales, but that are still very similar from the model perspective.\n\nImplementing this strategy while training a Siamese Neural Network is what makes the largest contribution to the model accuracy. Other details contribute somewhat to the accuracy, but have a much smaller impact."},{"metadata":{"_uuid":"bf399701be9c6b8d72fc05cccdefdc4f5eeb2a89"},"cell_type":"markdown","source":"# Overview\nThis notebook describes all the different elements of the submission. Obviously, to cover everything, it has to be fairly long. I encourage everyone to skip ahead directly  to whatever you are most interested in, without  necessarily going through everything.\n## Content\n1. Duplicate image identification (not much to see here -- keep moving)\n1. Image preprocessing (just the regular stuff)\n1. Siamese Neural Network architecture (some interesting thoughts)\n1. Training data construction (most of the secret sauce is here)\n1. Training procedure (zzzzz.....)\n1. Generating the submission file (re-zzzzz.....)\n1. Bootstrapping and ensemble (classic but short)\n1. Visualization (everyone's favorite!)\n1. Off topic (why add this unless it is interesting?)"},{"metadata":{"_uuid":"7afad70768ff847794be2ab8312d7fa04ca1c571","collapsed":true},"cell_type":"markdown","source":"# Duplicate image identification\nThis section describes the heuristic used to identify duplicate images. The fact that the training and test set have duplicate images has already been well documented. Some images are perfect binary copies, while other have been altered somewhat: contrast and brightness, size, masking the legend, etc. \n\nTwo images are considered duplicate if they meet the following criteria:\n\n1. Both images have the same [Perceptual Hash](http://www.phash.org/) (phash); or\n1. Both images have:\n    1. phash that differ by at most 6 bits, and;\n    1. have the same size, and;\n    1. the pixelwise mean square error between the normalized images is below a given threshold.\n\nThe *p2h* dictionary associate a unique image id (phash) for each picture. The *h2p* dictionary associate each unique image id to the prefered image to be used for this hash.\n\nThe prefered image is the one with the highest resolution, or any one if they have the same resolution."},{"metadata":{"_uuid":"34d1be7393ae2c49babbb540fc766fbc3274c579","trusted":true},"cell_type":"code","source":"# Read the dataset description\nfrom pandas import read_csv\n\ntagged = dict([(p,w) for _,p,w in read_csv('../input/whale-categorization-playground/train.csv').to_records()])\nsubmit = [p for _,p,_ in read_csv('../input/whale-categorization-playground/sample_submission.csv').to_records()]\njoin   = list(tagged.keys()) + submit\nlen(tagged),len(submit),len(join),list(tagged.items())[:5],submit[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab6d0c498190ba97c62b96cca930a1ba2201f987","trusted":true},"cell_type":"code","source":"# Determise the size of each image\nfrom os.path import isfile\nfrom PIL import Image as pil_image\nfrom tqdm import tqdm_notebook\n\ndef expand_path(p):\n    if isfile('../input/whale-categorization-playground/train/' + p): return '../input/whale-categorization-playground/train/' + p\n    if isfile('../input/whale-categorization-playground/test/' + p): return '../input/whale-categorization-playground/test/' + p\n    return p\n\np2size = {}\nfor p in tqdm_notebook(join):\n    size      = pil_image.open(expand_path(p)).size\n    p2size[p] = size\nlen(p2size), list(p2size.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"885c2c1714e2ef38c2c30ec441684ec853d0e1a0","trusted":true},"cell_type":"code","source":"# Read or generate p2h, a dictionary of image name to image id (picture to hash)\nimport pickle\nimport numpy as np\nfrom imagehash import phash\nfrom math import sqrt\n\n# Two phash values are considered duplicate if, for all associated image pairs:\n# 1) They have the same mode and size;\n# 2) After normalizing the pixel to zero mean and variance 1.0, the mean square error does not exceed 0.1\ndef match(h1,h2):\n    for p1 in h2ps[h1]:\n        for p2 in h2ps[h2]:\n            i1 =  pil_image.open(expand_path(p1))\n            i2 =  pil_image.open(expand_path(p2))\n            if i1.mode != i2.mode or i1.size != i2.size: return False\n            a1 = np.array(i1)\n            a1 = a1 - a1.mean()\n            a1 = a1/sqrt((a1**2).mean())\n            a2 = np.array(i2)\n            a2 = a2 - a2.mean()\n            a2 = a2/sqrt((a2**2).mean())\n            a  = ((a1 - a2)**2).mean()\n            if a > 0.1: return False\n    return True\n\nif isfile('../input/humpback-whale-identification-model-files/p2h.pickle'):\n    with open('../input/humpback-whale-identification-model-files/p2h.pickle', 'rb') as f:\n        p2h = pickle.load(f)\nelse:\n    # Compute phash for each image in the training and test set.\n    p2h = {}\n    for p in tqdm_notebook(join):\n        img    = pil_image.open(expand_path(p))\n        h      = phash(img)\n        p2h[p] = h\n\n    # Find all images associated with a given phash value.\n    h2ps = {}\n    for p,h in p2h.items():\n        if h not in h2ps: h2ps[h] = []\n        if p not in h2ps[h]: h2ps[h].append(p)\n\n    # Find all distinct phash values\n    hs = list(h2ps.keys())\n\n    # If the images are close enough, associate the two phash values (this is the slow part: n^2 algorithm)\n    h2h = {}\n    for i,h1 in enumerate(tqdm_notebook(hs)):\n        for h2 in hs[:i]:\n            if h1-h2 <= 6 and match(h1, h2):\n                s1 = str(h1)\n                s2 = str(h2)\n                if s1 < s2: s1,s2 = s2,s1\n                h2h[s1] = s2\n\n    # Group together images with equivalent phash, and replace by string format of phash (faster and more readable)\n    for p,h in p2h.items():\n        h = str(h)\n        if h in h2h: h = h2h[h]\n        p2h[p] = h\n\nlen(p2h), list(p2h.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61e410583127029513732341a99145151e329475","trusted":true},"cell_type":"code","source":"# For each image id, determine the list of pictures\nh2ps = {}\nfor p,h in p2h.items():\n    if h not in h2ps: h2ps[h] = []\n    if p not in h2ps[h]: h2ps[h].append(p)\n# Notice how 25460 images use only 20913 distinct image ids.\nlen(h2ps),list(h2ps.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e768fbcafade78a656c42a1db326cdd7c24478d","trusted":true},"cell_type":"code","source":"# Show an example of a duplicate image (from training of test set)\nimport matplotlib.pyplot as plt\n\ndef show_whale(imgs, per_row=2):\n    n         = len(imgs)\n    rows      = (n + per_row - 1)//per_row\n    cols      = min(per_row, n)\n    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n    for ax in axes.flatten(): ax.axis('off')\n    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n\nfor h, ps in h2ps.items():\n    if len(ps) > 2:\n        print('Images:', ps)\n        imgs = [pil_image.open(expand_path(p)) for p in ps]\n        show_whale(imgs, per_row=len(ps))\n        break","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# For each images id, select the prefered image\ndef prefer(ps):\n    if len(ps) == 1: return ps[0]\n    best_p = ps[0]\n    best_s = p2size[best_p]\n    for i in range(1, len(ps)):\n        p = ps[i]\n        s = p2size[p]\n        if s[0]*s[1] > best_s[0]*best_s[1]: # Select the image with highest resolution\n            best_p = p\n            best_s = s\n    return best_p\n\nh2p = {}\nfor h,ps in h2ps.items(): h2p[h] = prefer(ps)\nlen(h2p),list(h2p.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd6640560036cf8e6d581a204e4a78a96f465d36","collapsed":true},"cell_type":"markdown","source":"# Image preprocessing\nTraining is performed on images subjected to the following operations:\n\n1.  Rotate the image if it is in the rotate set;\n1. Transform to black and white;\n1. Apply an affine transformation ."},{"metadata":{"_uuid":"aec39ab5192624f08c55a330cf04804b0d63b7c1"},"cell_type":"markdown","source":"## Image rotation"},{"metadata":{"_uuid":"5275aba3332f8600731d19e9b9a5c3cea696e415"},"cell_type":"markdown","source":"I noticed that some pictures have the whale fluke pointing down instead of up as usual. Whenever I encountered such instance in the training set (not in the test set), I would add it to a list. During training, these images are rotated 180 degrees to normalize them with the fluke pointing up. The list is not exhausitve, there are probably more case that I have not noticed."},{"metadata":{"_uuid":"237fba51c8e460e2c362a8876ef963a37b60bbab","trusted":true},"cell_type":"code","source":"with open('../input/humpback-whale-identification-model-files/rotate.txt', 'rt') as f: rotate = f.read().split('\\n')[:-1]\nrotate = set(rotate)\nrotate","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4687ab632f33f5afc7d2f813a3be441927cd9ae","trusted":true},"cell_type":"code","source":"def read_raw_image(p):\n    img = pil_image.open(expand_path(p))\n    if p in rotate: img = img.rotate(180)\n    return img\n\np    = list(rotate)[0]\nimgs = [pil_image.open(expand_path(p)), read_raw_image(p)]\nshow_whale(imgs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9fb058e68b8d9c87426e46670e06e29fab8a85a"},"cell_type":"markdown","source":"The left side image is the original image (fluke pointing down). The right side image is rotated 180 degrees."},{"metadata":{"_uuid":"53bfdd79fc980891a85a6bd93dc54ef5b25a8d98"},"cell_type":"markdown","source":"## Convert to black and white\nIn my early experiments, I noticed that my models achieved approximately the same accuracy when comparing two colored images, or two black and white images. However, comparing a colored image with a black and white image resulted in much lower accuracy. The simplest solution was to convert all images to black and white, which did not reduce the accuracy even when comparing originally colored images."},{"metadata":{"_uuid":"c44ac944607ebb8bfed66324e60440fbda4bb8fc"},"cell_type":"markdown","source":"## Affine tranformation\nThe [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) maps a rectangular area of the original image to a square image with resolution 384x384x1 (only one channel for black and white). The rectangular area has a width over height aspect ratio of 2.15, close to the average picture aspect ratio. The rectangle is taken to be slightly larger than the computed [bounding box](https://www.kaggle.com/martinpiotte/bounding-box-model), as computed in the model defined in a separate kernel, The idea is that clipping the edges of the fluke is more harmful than the gain obtained by fitting it exactly, thus a margin is preferred.\n\nDuring training, data augmentation is performed by adding a random transformation that composes zoom, shift, rotation and shear. The random transform is skipped when testing.\n\nFinally, the image is normalized to zero mean and unit variance."},{"metadata":{"_uuid":"b1b9bff9cbec6d6120abd1d11a94929481780acb","trusted":true},"cell_type":"code","source":"# Read the bounding box data from the bounding box kernel (see reference above)\nwith open('../input/humpback-whale-identification-model-files/bounding-box.pickle', 'rb') as f:\n    p2bb = pickle.load(f)\nlist(p2bb.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c4192e165c301719289053cd8e66ce1cd2367d","trusted":true},"cell_type":"code","source":"# Suppress annoying stderr output when importing keras.\nimport sys\nimport platform\nold_stderr = sys.stderr\nsys.stderr = open('/dev/null' if platform.system() != 'Windows' else 'nul', 'w')\nimport keras\nsys.stderr = old_stderr\n\nimport random\nfrom keras import backend as K\nfrom keras.preprocessing.image import img_to_array,array_to_img\nfrom scipy.ndimage import affine_transform\n\nimg_shape    = (384,384,1) # The image shape used by the model\nanisotropy   = 2.15 # The horizontal compression ratio\ncrop_margin  = 0.05 # The margin added around the bounding box to compensate for bounding box inaccuracy\n\ndef build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    \"\"\"\n    Build a transformation matrix with the specified characteristics.\n    \"\"\"\n    rotation        = np.deg2rad(rotation)\n    shear           = np.deg2rad(shear)\n    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n\ndef read_cropped_image(p, augment):\n    \"\"\"\n    @param p : the name of the picture to read\n    @param augment: True/False if data augmentation should be performed\n    @return a numpy array with the transformed image\n    \"\"\"\n    # If an image id was given, convert to filename\n    if p in h2p: p = h2p[p]\n    size_x,size_y = p2size[p]\n    \n    # Determine the region of the original image we want to capture based on the bounding box.\n    x0,y0,x1,y1   = p2bb[p]\n    if p in rotate: x0, y0, x1, y1 = size_x - x1, size_y - y1, size_x - x0, size_y - y0\n    dx            = x1 - x0\n    dy            = y1 - y0\n    x0           -= dx*crop_margin\n    x1           += dx*crop_margin + 1\n    y0           -= dy*crop_margin\n    y1           += dy*crop_margin + 1\n    if (x0 < 0     ): x0 = 0\n    if (x1 > size_x): x1 = size_x\n    if (y0 < 0     ): y0 = 0\n    if (y1 > size_y): y1 = size_y\n    dx            = x1 - x0\n    dy            = y1 - y0\n    if dx > dy*anisotropy:\n        dy  = 0.5*(dx/anisotropy - dy)\n        y0 -= dy\n        y1 += dy\n    else:\n        dx  = 0.5*(dy*anisotropy - dx)\n        x0 -= dx\n        x1 += dx\n\n    # Generate the transformation matrix\n    trans = np.array([[1, 0, -0.5*img_shape[0]], [0, 1, -0.5*img_shape[1]], [0, 0, 1]])\n    trans = np.dot(np.array([[(y1 - y0)/img_shape[0], 0, 0], [0, (x1 - x0)/img_shape[1], 0], [0, 0, 1]]), trans)\n    if augment:\n        trans = np.dot(build_transform(\n            random.uniform(-5, 5),\n            random.uniform(-5, 5),\n            random.uniform(0.8, 1.0),\n            random.uniform(0.8, 1.0),\n            random.uniform(-0.05*(y1 - y0), 0.05*(y1 - y0)),\n            random.uniform(-0.05*(x1 - x0), 0.05*(x1 - x0))\n            ), trans)\n    trans = np.dot(np.array([[1, 0, 0.5*(y1 + y0)], [0, 1, 0.5*(x1 + x0)], [0, 0, 1]]), trans)\n\n    # Read the image, transform to black and white and comvert to numpy array\n    img   = read_raw_image(p).convert('L')\n    img   = img_to_array(img)\n    \n    # Apply affine transformation\n    matrix = trans[:2,:2]\n    offset = trans[:2,2]\n    img    = img.reshape(img.shape[:-1])\n    img    = affine_transform(img, matrix, offset, output_shape=img_shape[:-1], order=1, mode='constant', cval=np.average(img))\n    img    = img.reshape(img_shape)\n\n    # Normalize to zero mean and unit variance\n    img  -= np.mean(img, keepdims=True)\n    img  /= np.std(img, keepdims=True) + K.epsilon()\n    return img\n\ndef read_for_training(p):\n    \"\"\"\n    Read and preprocess an image with data augmentation (random transform).\n    \"\"\"\n    return read_cropped_image(p, True)\n\ndef read_for_validation(p):\n    \"\"\"\n    Read and preprocess an image without data augmentation (use for testing).\n    \"\"\"\n    return read_cropped_image(p, False)\n\np = list(tagged.keys())[312]\nimgs = [\n    read_raw_image(p),\n    array_to_img(read_for_validation(p)),\n    array_to_img(read_for_training(p))\n]\nshow_whale(imgs, per_row=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e671c843a32506b635f92e75a7574346fe339254"},"cell_type":"markdown","source":"The left image is the original picture. The center image does the test transformation. The right image adds a random data augmentation transformation."},{"metadata":{"_uuid":"4c35fc67c23a1677d37fe4766cd3fb3e14b840d9"},"cell_type":"markdown","source":"# Siamese Neural Network architecture\nThe Siamese Neural Network compares two images and decides if the two images are taken from the same whale, or different whales. By testing each image from the test set against every image from the training set, the most likely whales can be identified by sorting the pictures in likelihood for a match.\n\nA Siamese Neural Network is composed of two parts. A Convolutation Neural Network (CNN) transforms an input image into a vector of features describing the whale. The same CNN, with the same weights, is used for both images. I call the CNN the __branch model__. I used a custom model mostly inspired from [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf).\n\nWhat I call the __head model__ is used to compare the feature vectors from the CNN and decide if the whales match or not."},{"metadata":{"_uuid":"f7fbe3e42597f837363c6cdebb5a8853d5c68399"},"cell_type":"markdown","source":"## Head model\nThe head model compares the feature vector from the branch model to decide if the pictures show the same or different whales.  The typical approach is to use a distance measure (e.g. \\\\( L_1 \\\\)) with a contrastive loss function, but there are a few reasons to try something different:\n\n* A distance measure will consider two features with value zero as a perfect match, while two features with large but slightly different values will be seen as good, but not quite as good since they are not exactly equal. Still, I feel there is more postive signal in the active features than in the negative ones, especially with ReLU (Rectified Linear Unit) activation, concept that is lost by the distance measure.\n* Also, a distance measure does not provide for features to be negatively correlated. Consider a case where, if both images have feature X, they must be the same whale, unless they also both have feature Y, in which case X is not as clear.\n* At the same time, there is this implicit assumption that swapping the two images must produce the same result: if A is the same whale as B, B must be the same whale as A. \n\nTo address these concerns, i proceed as follow:\n\n1. For each feature I compute the sum, the product, the absolute difference and the difference squared (\\\\(x + y, x y, |x - y|, (x - y)^2\\\\)).\n1. The four values are passed through a small neural network, which can learn how to weigh between matching zeros and close non-zero values. The same neural net with the same weights is used for each feature.\n1. The output is a weighted sum of the converted features, with a sigmoid activation. The value of the weight is redundant, because the weight is just a scaling factor of the feature, which could be learned by another layer, however, it allows for negative weights, which cannot be produced otherwise when using ReLU activation."},{"metadata":{"_uuid":"0ba49143534d8fee4d9d7adeee7a912736c91e45"},"cell_type":"markdown","source":"## Branch model\nThe branch model is a regular CNN model. Here are the key elements of its design:\n* Because the training dataset is small, I tried to keep the number of learned parameter relatively small, while keeping the model expressive enough. A ResNet like architecture is more economical than a [VGG](https://arxiv.org/abs/1409.1556) like network for example.\n* The problem turned out to be memory bound, with most memory being taken to store activations from the feedforward pass, used to compute the gradient during backpropagation. With Windows 10 and a GTX 1080 card, there is about 6.8GB VRAM available, and this limit has constrained the model choice. \n\nThe branch model is composed of 6 blocks, each block processing maps with smaller and smaller resolution,, with intermediate pooling layers.\n\n* Block 1 - 384x384\n* Block 2 - 96x96\n* Block 3 - 48x48\n* Block 4 - 24x24\n* Block 5 - 12x12\n* Block 6 - 6x6\n\nBlock 1 has a single convolution layer with stride 2 followed by 2x2 max pooling. Because of the high resolution, it uses a lot of memory, so a minimum of work is done here to save memory for subsequent blocks.\n\nBlock 2 has two 3x3 convolutions similar to VGG. These convolutions are less memory intensive then the subsequent ResNet blocks, and are used to save memory. Note that after this, the tensor has dimension 96x96x64, the same volume as the initial 384x384x1 image, thus we can assume no significant information has been lost.\n\nBlocks 3 to 6 perform ResNet like convolution. I suggest reading the original paper, but the idea is to form a subblock with a 1x1 convolution reducing the number of features, a 3x3 convolution and another 1x1 convolution to restore the number of features to the original. The output of these convolutions is then added to the original tensor (bypass connection). I use 4 such subblocks by block, plus a single 1x1 convolution to increase the feature count after each pooling layer.\n\nThe final step of the branch model is a global max pooling, which makes the model robust to fluke not being always well centered."},{"metadata":{"_uuid":"9f11d92483b771073da6c1a2f0eb62815e92aee0"},"cell_type":"markdown","source":"## Code\nThe following is the Keras code for the model."},{"metadata":{"_uuid":"7f90b807f6236490117f67b485bb2268b32eb8b4","scrolled":true,"trusted":true},"cell_type":"code","source":"from keras import regularizers\nfrom keras.optimizers import Adam\nfrom keras.engine.topology import Input\nfrom keras.layers import Activation, Add, BatchNormalization, Concatenate, Conv2D, Dense, Flatten, GlobalMaxPooling2D, Lambda, MaxPooling2D, Reshape\nfrom keras.models import Model\n\ndef subblock(x, filter, **kwargs):\n    x = BatchNormalization()(x)\n    y = x\n    y = Conv2D(filter, (1, 1), activation='relu', **kwargs)(y) # Reduce the number of features to 'filter'\n    y = BatchNormalization()(y)\n    y = Conv2D(filter, (3, 3), activation='relu', **kwargs)(y) # Extend the feature field\n    y = BatchNormalization()(y)\n    y = Conv2D(K.int_shape(x)[-1], (1, 1), **kwargs)(y) # no activation # Restore the number of original features\n    y = Add()([x,y]) # Add the bypass connection\n    y = Activation('relu')(y)\n    return y\n\ndef build_model(lr, l2, activation='sigmoid'):\n\n    ##############\n    # BRANCH MODEL\n    ##############\n    regul  = regularizers.l2(l2)\n    optim  = Adam(lr=lr)\n    kwargs = {'padding':'same', 'kernel_regularizer':regul}\n\n    inp = Input(shape=img_shape) # 384x384x1\n    x   = Conv2D(64, (9,9), strides=2, activation='relu', **kwargs)(inp)\n\n    x   = MaxPooling2D((2, 2), strides=(2, 2))(x) # 96x96x64\n    for _ in range(2):\n        x = BatchNormalization()(x)\n        x = Conv2D(64, (3,3), activation='relu', **kwargs)(x)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 48x48x64\n    x = BatchNormalization()(x)\n    x = Conv2D(128, (1,1), activation='relu', **kwargs)(x) # 48x48x128\n    for _ in range(4): x = subblock(x, 64, **kwargs)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 24x24x128\n    x = BatchNormalization()(x)\n    x = Conv2D(256, (1,1), activation='relu', **kwargs)(x) # 24x24x256\n    for _ in range(4): x = subblock(x, 64, **kwargs)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 12x12x256\n    x = BatchNormalization()(x)\n    x = Conv2D(384, (1,1), activation='relu', **kwargs)(x) # 12x12x384\n    for _ in range(4): x = subblock(x, 96, **kwargs)\n\n    x = MaxPooling2D((2, 2), strides=(2, 2))(x) # 6x6x384\n    x = BatchNormalization()(x)\n    x = Conv2D(512, (1,1), activation='relu', **kwargs)(x) # 6x6x512\n    for _ in range(4): x = subblock(x, 128, **kwargs)\n    \n    x             = GlobalMaxPooling2D()(x) # 512\n    branch_model  = Model(inp, x)\n    \n    ############\n    # HEAD MODEL\n    ############\n    mid        = 32\n    xa_inp     = Input(shape=branch_model.output_shape[1:])\n    xb_inp     = Input(shape=branch_model.output_shape[1:])\n    x1         = Lambda(lambda x : x[0]*x[1])([xa_inp, xb_inp])\n    x2         = Lambda(lambda x : x[0] + x[1])([xa_inp, xb_inp])\n    x3         = Lambda(lambda x : K.abs(x[0] - x[1]))([xa_inp, xb_inp])\n    x4         = Lambda(lambda x : K.square(x))(x3)\n    x          = Concatenate()([x1, x2, x3, x4])\n    x          = Reshape((4, branch_model.output_shape[1], 1), name='reshape1')(x)\n\n    # Per feature NN with shared weight is implemented using CONV2D with appropriate stride.\n    x          = Conv2D(mid, (4, 1), activation='relu', padding='valid')(x)\n    x          = Reshape((branch_model.output_shape[1], mid, 1))(x)\n    x          = Conv2D(1, (1, mid), activation='linear', padding='valid')(x)\n    x          = Flatten(name='flatten')(x)\n    \n    # Weighted sum implemented as a Dense layer.\n    x          = Dense(1, use_bias=True, activation=activation, name='weighted-average')(x)\n    head_model = Model([xa_inp, xb_inp], x, name='head')\n\n    ########################\n    # SIAMESE NEURAL NETWORK\n    ########################\n    # Complete model is constructed by calling the branch model on each input image,\n    # and then the head model on the resulting 512-vectors.\n    img_a      = Input(shape=img_shape)\n    img_b      = Input(shape=img_shape)\n    xa         = branch_model(img_a)\n    xb         = branch_model(img_b)\n    x          = head_model([xa, xb])\n    model      = Model([img_a, img_b], x)\n    model.compile(optim, loss='binary_crossentropy', metrics=['binary_crossentropy', 'acc'])\n    return model, branch_model, head_model\n\nmodel, branch_model, head_model = build_model(64e-5,0)\nhead_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7784a525bdebc38d2b89b106f33c2893e58ebf04","trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(head_model, to_file='head-model.png')\npil_image.open('head-model.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0efc2c31c8d2a1aa8aef5d135439d8cf2e5489d2","trusted":true},"cell_type":"code","source":"branch_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16cb29a5f2fe1e1923f51ec5207297dfeb69c216","trusted":true},"cell_type":"code","source":"# Oops, this is HUGE!\nplot_model(branch_model, to_file='branch-model.png')\nimg = pil_image.open('branch-model.png')\nimg.resize([x//2 for x in img.size])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f6cb609b15e172955f634c8f68f15a98d6944b0","collapsed":true},"cell_type":"markdown","source":"# Training data construction\nAs highlighted in the abstract section, this is the part that makes a big difference in the model accuracy.\n\nWe want the Siamese Neural Network to pick the one correct whale from all the possible whales from the training set. While scoring the correct whale high, it must simultaneously score **all** other whales lower. It is not enough to have a random whale score low. To force all the other whales to a low probability, the training algorithm presents pairs of pictures with increasing difficulty, as evaluated by the model at any given time. Essentially, we forcus the training on pairs that he model is getting wrong, as a form of adversarial training.\n\nAt the same time, we want the model to recognize **whales** and not **pictures**. Given the small number of pictures in the training dataset, it is not irrealistic to imagine the model recognizing a specific picture using the shape of a wave, or a bird flying by. To prevent this, the data presented to the model must be unbiased. If a picture is used more often in negative examples, the model risks simply learning to guess a mismatch whenever this picture is present, without learning how to compare the whales correctly. By presenting each image an equal number of times, with 50% positive and 50% negative examples, the model has no incentive in learning to recognize specific pictures, and thus focuses on recognizing whales as desired."},{"metadata":{"_uuid":"7e04e1c924396cda1834e675b2de8a447267116d"},"cell_type":"markdown","source":"## Image selection\nTo begin, we reduce the number of images from the training set:\n\n* Images from the blacklist are removed;\n* Duplicate images are removed;\n* All 'new_whale' images are removed;\n* All whales with a single image are removed.\n\nThe blacklist was constructed manually by spotting images unhelpful to training. Reasons could be the underside of the fluke is not visible, or we see only dead fluke fragments on the a beach, there are two whales in the picture, etc. The list is in no way exhaustive."},{"metadata":{"_uuid":"a3403d1d59eb66f496f042c0a08b3286814b2117","trusted":true},"cell_type":"code","source":"with open('../input/humpback-whale-identification-model-files/exclude.txt', 'rt') as f: exclude = f.read().split('\\n')[:-1]   \nlen(exclude)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edca49e4b9e795a202c426804104d0ac917905c0","trusted":true},"cell_type":"code","source":"show_whale([read_raw_image(p) for p in exclude], per_row=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4edecade09fba068c488c50b86f08a59417cba74","collapsed":true},"cell_type":"markdown","source":"## Matching whale examples\nHalf the examples used during training are for pair of images. For each whale of the training set, compute a [derangement](https://en.wikipedia.org/wiki/Derangement) of its pictures. Use the original order as picture A, and the derangment as picture B. This creates a random number of matching image pairs, with each image taken exactly two times."},{"metadata":{"_uuid":"0fd59cc7ab45460467f1df0fc32c75e089d351f8","collapsed":true},"cell_type":"markdown","source":"## Different whales examples"},{"metadata":{"_uuid":"ecfa5542699ab68b70078ef65feaba5d4c0fcac2"},"cell_type":"markdown","source":"Different whales examples are generated by computing a derangement of all pictures from the training set, subject to:\n\n* The image pairs must belong to different whales;\n* The pairs formed must be difficult for the model to distinguish.\n\nThe following algorith is used to generate the pairs:\n\n1. The similarity between each pair of image is computed using the current model state. This has complexity \\\\( \\frac {n(n-1)} 2  \\\\), where n is the size of the training set. Fortunately, only the head model must be computed for all pairs, and it is very fast. The 512-feature vectors can be pre-computed once for each image, i.e. O(n) complexity.\n1. Entry that correpond to pair of images from the same whale are set with similarity \\\\( -\\infty \\\\).\n1. [Linear sum assignment](https://en.wikipedia.org/wiki/Assignment_problem) algorithm is used to find the most difficult matching. \n1. To randomize the selection, and control the matching difficulty, we add a random matrix to the cost matrix from step #1 . The random matrix has values uniformly distributed between 0 and K. The larger K, the more random the matching. The smaller K, the more difficult the pairing is for the model.\n1. To produce different matching for successive epochs, the selected entries in the matrix are overwritten with \\\\( -\\infty \\\\) to force an alternate selection for the next matching."},{"metadata":{"_uuid":"70ba1841da5b8d6d51ce06734d4405ef4c129d6f"},"cell_type":"markdown","source":"## Code\nThe described logic is essentially implemented by the TrainingData class, that performs just in time data augmentation as well as computing the matching."},{"metadata":{"_uuid":"4b0b2120d9f3d38bcc0971e0c0baab3e07dcbbcc","trusted":true},"cell_type":"code","source":"# Find all the whales associated with an image id. It can be ambiguous as duplicate images may have different whale ids.\nh2ws = {}\nnew_whale = 'new_whale'\nfor p,w in tagged.items():\n    if w != new_whale: # Use only identified whales\n        h = p2h[p]\n        if h not in h2ws: h2ws[h] = []\n        if w not in h2ws[h]: h2ws[h].append(w)\nfor h,ws in h2ws.items():\n    if len(ws) > 1:\n        h2ws[h] = sorted(ws)\nlen(h2ws)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8844f37d5d2c40eb9347dfb29b2383ecdf417557","trusted":true},"cell_type":"code","source":"# For each whale, find the unambiguous images ids.\nw2hs = {}\nfor h,ws in h2ws.items():\n    if len(ws) == 1: # Use only unambiguous pictures\n        if h2p[h] in exclude:\n            print(h) # Skip excluded images\n        else:\n            w = ws[0]\n            if w not in w2hs: w2hs[w] = []\n            if h not in w2hs[w]: w2hs[w].append(h)\nfor w,hs in w2hs.items():\n    if len(hs) > 1:\n        w2hs[w] = sorted(hs)\nlen(w2hs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1420dd75317af6d7876f86187713e28e4cd385d","trusted":true},"cell_type":"code","source":"# Find the list of training images, keep only whales with at least two images.\ntrain = [] # A list of training image ids\nfor hs in w2hs.values():\n    if len(hs) > 1:\n        train += hs\nrandom.shuffle(train)\ntrain_set = set(train)\n\nw2ts = {} # Associate the image ids from train to each whale id.\nfor w,hs in w2hs.items():\n    for h in hs:\n        if h in train_set:\n            if w not in w2ts: w2ts[w] = []\n            if h not in w2ts[w]: w2ts[w].append(h)\nfor w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n    \nt2i = {} # The position in train of each training image id\nfor i,t in enumerate(train): t2i[t] = i\n\nlen(train),len(w2ts)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bce8abd37c4ccdcc0059504c03e2d4f3371fd170","trusted":true},"cell_type":"code","source":"from keras.utils import Sequence\n\n# First try to use lapjv Linear Assignment Problem solver as it is much faster.\n# At the time I am writing this, kaggle kernel with custom package fail to commit.\n# scipy can be used as a fallback, but it is too slow to run this kernel under the time limit\n# As a workaround, use scipy with data partitioning.\n# Because algorithm is O(n^3), small partitions are much faster, but not what produced the submitted solution\ntry:\n    from lap import lapjv\n    segment = False\nexcept ImportError:\n    print('Module lap not found, emulating with much slower scipy.optimize.linear_sum_assignment')\n    segment = True\n    from scipy.optimize import linear_sum_assignment\n\nclass TrainingData(Sequence):\n    def __init__(self, score, steps=1000, batch_size=32):\n        \"\"\"\n        @param score the cost matrix for the picture matching\n        @param steps the number of epoch we are planning with this score matrix\n        \"\"\"\n        super(TrainingData, self).__init__()\n        self.score      = -score # Maximizing the score is the same as minimuzing -score.\n        self.steps      = steps\n        self.batch_size = batch_size\n        for ts in w2ts.values():\n            idxs = [t2i[t] for t in ts]\n            for i in idxs:\n                for j in idxs:\n                    self.score[i,j] = 10000.0 # Set a large value for matching whales -- eliminates this potential pairing\n        self.on_epoch_end()\n    def __getitem__(self, index):\n        start = self.batch_size*index\n        end   = min(start + self.batch_size, len(self.match) + len(self.unmatch))\n        size  = end - start\n        assert size > 0\n        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n        b     = np.zeros((size,) + img_shape, dtype=K.floatx())\n        c     = np.zeros((size,1), dtype=K.floatx())\n        j     = start//2\n        for i in range(0, size, 2):\n            a[i,  :,:,:] = read_for_training(self.match[j][0])\n            b[i,  :,:,:] = read_for_training(self.match[j][1])\n            c[i,  0    ] = 1 # This is a match\n            a[i+1,:,:,:] = read_for_training(self.unmatch[j][0])\n            b[i+1,:,:,:] = read_for_training(self.unmatch[j][1])\n            c[i+1,0    ] = 0 # Different whales\n            j           += 1\n        return [a,b],c\n    def on_epoch_end(self):\n        if self.steps <= 0: return # Skip this on the last epoch.\n        self.steps     -= 1\n        self.match      = []\n        self.unmatch    = []\n        if segment:\n            # Using slow scipy. Make small batches.\n            # Because algorithm is O(n^3), small batches are much faster.\n            # However, this does not find the real optimum, just an approximation.\n            tmp   = []\n            batch = 512\n            for start in range(0, score.shape[0], batch):\n                end = min(score.shape[0], start + batch)\n                _, x = linear_sum_assignment(self.score[start:end, start:end])\n                tmp.append(x + start)\n            x = np.concatenate(tmp)\n        else:\n            _,_,x = lapjv(self.score) # Solve the linear assignment problem\n        y = np.arange(len(x),dtype=np.int32)\n\n        # Compute a derangement for matching whales\n        for ts in w2ts.values():\n            d = ts.copy()\n            while True:\n                random.shuffle(d)\n                if not np.any(ts == d): break\n            for ab in zip(ts,d): self.match.append(ab)\n\n        # Construct unmatched whale pairs from the LAP solution.\n        for i,j in zip(x,y):\n            if i == j:\n                print(self.score)\n                print(x)\n                print(y)\n                print(i,j)\n            assert i != j\n            self.unmatch.append((train[i],train[j]))\n\n        # Force a different choice for an eventual next epoch.\n        self.score[x,y] = 10000.0\n        self.score[y,x] = 10000.0\n        random.shuffle(self.match)\n        random.shuffle(self.unmatch)\n        # print(len(self.match), len(train), len(self.unmatch), len(train))\n        assert len(self.match) == len(train) and len(self.unmatch) == len(train)\n    def __len__(self):\n        return (len(self.match) + len(self.unmatch) + self.batch_size - 1)//self.batch_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38a2392b38554486c1396ecf9614c3785c03c08c","trusted":true},"cell_type":"code","source":"# Test on a batch of 32 with random costs.\nscore = np.random.random_sample(size=(len(train),len(train)))\ndata = TrainingData(score)\n(a, b), c = data[0]\na.shape, b.shape, c.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54c10e3e139a93c59a125a50329d59ef6059083b","trusted":true},"cell_type":"code","source":"# First pair is for matching whale\nimgs = [array_to_img(a[0]), array_to_img(b[0])]\nshow_whale(imgs, per_row=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63da9a872cbea6641fb7a8740696b5b559fa38f9","trusted":true},"cell_type":"code","source":"# Second pair is for different whales\nimgs = [array_to_img(a[1]), array_to_img(b[1])]\nshow_whale(imgs, per_row=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6571f015c52491684f8af4cf88bdaebbd09b018c","collapsed":true},"cell_type":"markdown","source":"# Training procedure\nThis scection describes the procedure used to train the model. Training lasts 400 epochs, with the following quantities changing as the training progresses:\n\n* Learning rate\n* L2 regularization\n* Constant K measuring the scale of the random component of the score matrix used to match similar images to construct tough training cases.\n\nThe procedure itself has evolved from many previous experiments, trial and errors on earlier versions of the model.\n\nTraining the large model from random weights is difficult. In fact, if the model is initially fed examples that are too hard, it does not converge at all. In the context here, hard examples are similar images belonging to different whales. Pushed to the extreme, it it possible to construct a training dataset for which pairs of pictures of different whales appear (to the model) more similar than pairs of pictures from the same whale, making the model learn to classify similar images as different whales, and dissimilar images as same whales!\n\nTo prevent this, early training is executed with a large value of K, making the negative examples essentially random pairs of pictures of different whales. As the model ability to distinguish between whales increases, K is gradually reduced, presenting harder training cases. Similarly, training starts with no L2 regularization. After 250 epochs, trainings accuracy is fantastic, but it also grossly overfits. At this point, L2 regularization is applied, learning rate is reset to a large value and training continues for an additional 150 epochs."},{"metadata":{"_uuid":"0a3e377c8cb687a83799c0f1a123b7216c08495c"},"cell_type":"markdown","source":"The following table shows the exact schedule used for the learning rate (LR), L2 regularization (L2) and randomized score matrix (K). \n\nAlso note that the score matrix for the Linear Assignment Problem is computed at every 5 epochs starting with epoch 10.\n\nEpochs | LR | K | L2\n---- | ---: | ---: | ---:\n1-10 | \\\\( 64.10^{-5} \\\\)  | \\\\( +\\infty \\\\) | 0\n11-15 | \\\\( 64.10^{-5} \\\\) | 100.00 | 0\n16-20 | \\\\( 64.10^{-5} \\\\) | 63.10 | 0\n21-25 | \\\\( 64.10^{-5} \\\\) | 39.81 | 0\n26-30 | \\\\( 64.10^{-5} \\\\) | 25.12 | 0\n31-35 | \\\\( 64.10^{-5} \\\\) | 15.85 | 0\n36-40 | \\\\( 64.10^{-5} \\\\) | 10.00 | 0\n41-45 | \\\\( 64.10^{-5} \\\\) | 6.31 | 0\n46-50 | \\\\( 64.10^{-5} \\\\) | 3.98 | 0\n51-55 | \\\\( 64.10^{-5} \\\\) | 2.51 | 0\n56-60 | \\\\( 64.10^{-5} \\\\) | 1.58 | 0\n61-150 | \\\\( 64.10^{-5} \\\\) | 1.00 | 0\n151-200 | \\\\( 16.10^{-5} \\\\) | 0.50 | 0\n200-240 | \\\\( 4.10^{-5} \\\\) | 0.25 | 0\n241-250 | \\\\( 10^{-5} \\\\) | 0.25 | 0\n251-300 | \\\\( 64.10^{-5} \\\\) | 1.00 | \\\\( 2.10^{-4} \\\\)\n301-350 | \\\\( 16.10^{-5} \\\\) | 0.50 | \\\\( 2.10^{-4} \\\\)\n351-390 | \\\\( 4.10^{-5} \\\\) | 0.25 | \\\\( 2.10^{-4} \\\\)\n391-400 | \\\\( 10^{-5} \\\\) | 0.25 | \\\\( 2.10^{-4} \\\\)\n\n"},{"metadata":{"_uuid":"d8878aaebeb16efc57a1628dc47c0a811b7c9a6e","trusted":true,"collapsed":true},"cell_type":"code","source":"# A Keras generator to evaluate only the BRANCH MODEL\nclass FeatureGen(Sequence):\n    def __init__(self, data, batch_size=64, verbose=1):\n        super(FeatureGen, self).__init__()\n        self.data       = data\n        self.batch_size = batch_size\n        self.verbose    = verbose\n        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Features')\n    def __getitem__(self, index):\n        start = self.batch_size*index\n        size  = min(len(self.data) - start, self.batch_size)\n        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n        for i in range(size): a[i,:,:,:] = read_for_validation(self.data[start + i])\n        if self.verbose > 0: \n            self.progress.update()\n            if self.progress.n >= len(self): self.progress.close()\n        return a\n    def __len__(self):\n        return (len(self.data) + self.batch_size - 1)//self.batch_size\n\n# A Keras generator to evaluate on the HEAD MODEL on features already pre-computed.\n# It computes only the upper triangular matrix of the cost matrix if y is None.\nclass ScoreGen(Sequence):\n    def __init__(self, x, y=None, batch_size=2048, verbose=1):\n        super(ScoreGen, self).__init__()\n        self.x          = x\n        self.y          = y\n        self.batch_size = batch_size\n        self.verbose    = verbose\n        if y is None:\n            self.y           = self.x\n            self.ix, self.iy = np.triu_indices(x.shape[0],1)\n        else:\n            self.iy, self.ix = np.indices((y.shape[0],x.shape[0]))\n            self.ix          = self.ix.reshape((self.ix.size,))\n            self.iy          = self.iy.reshape((self.iy.size,))\n        self.subbatch = (len(self.x) + self.batch_size - 1)//self.batch_size\n        if self.verbose > 0: self.progress = tqdm_notebook(total=len(self), desc='Scores')\n    def __getitem__(self, index):\n        start = index*self.batch_size\n        end   = min(start + self.batch_size, len(self.ix))\n        a     = self.y[self.iy[start:end],:]\n        b     = self.x[self.ix[start:end],:]\n        if self.verbose > 0: \n            self.progress.update()\n            if self.progress.n >= len(self): self.progress.close()\n        return [a,b]\n    def __len__(self):\n        return (len(self.ix) + self.batch_size - 1)//self.batch_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0835cea78c2d2c63ef290d5ff826f3c2a866504","trusted":true,"collapsed":true},"cell_type":"code","source":"from keras_tqdm import TQDMNotebookCallback\n\ndef set_lr(model, lr):\n    K.set_value(model.optimizer.lr, float(lr))\n\ndef get_lr(model):\n    return K.get_value(model.optimizer.lr)\n\ndef score_reshape(score, x, y=None):\n    \"\"\"\n    Tranformed the packed matrix 'score' into a square matrix.\n    @param score the packed matrix\n    @param x the first image feature tensor\n    @param y the second image feature tensor if different from x\n    @result the square matrix\n    \"\"\"\n    if y is None:\n        # When y is None, score is a packed upper triangular matrix.\n        # Unpack, and transpose to form the symmetrical lower triangular matrix.\n        m = np.zeros((x.shape[0],x.shape[0]), dtype=K.floatx())\n        m[np.triu_indices(x.shape[0],1)] = score.squeeze()\n        m += m.transpose()\n    else:\n        m        = np.zeros((y.shape[0],x.shape[0]), dtype=K.floatx())\n        iy,ix    = np.indices((y.shape[0],x.shape[0]))\n        ix       = ix.reshape((ix.size,))\n        iy       = iy.reshape((iy.size,))\n        m[iy,ix] = score.squeeze()\n    return m\n\ndef compute_score(verbose=1):\n    \"\"\"\n    Compute the score matrix by scoring every pictures from the training set against every other picture O(n^2).\n    \"\"\"\n    features = branch_model.predict_generator(FeatureGen(train, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n    score    = head_model.predict_generator(ScoreGen(features, verbose=verbose), max_queue_size=12, workers=6, verbose=0)\n    score    = score_reshape(score, features)\n    return features, score\n\ndef make_steps(step, ampl):\n    \"\"\"\n    Perform training epochs\n    @param step Number of epochs to perform\n    @param ampl the K, the randomized component of the score matrix.\n    \"\"\"\n    global w2ts, t2i, steps, features, score, histories\n    \n    # shuffle the training pictures\n    random.shuffle(train)\n    \n    # Map whale id to the list of associated training picture hash value\n    w2ts = {}\n    for w,hs in w2hs.items():\n        for h in hs:\n            if h in train_set:\n                if w not in w2ts: w2ts[w] = []\n                if h not in w2ts[w]: w2ts[w].append(h)\n    for w,ts in w2ts.items(): w2ts[w] = np.array(ts)\n\n    # Map training picture hash value to index in 'train' array    \n    t2i  = {}\n    for i,t in enumerate(train): t2i[t] = i    \n\n    # Compute the match score for each picture pair\n    features, score = compute_score()\n    \n    # Train the model for 'step' epochs\n    history = model.fit_generator(\n        TrainingData(score + ampl*np.random.random_sample(size=score.shape), steps=step, batch_size=32),\n        initial_epoch=steps, epochs=steps + step, max_queue_size=12, workers=6, verbose=0,\n        callbacks=[\n            TQDMNotebookCallback(leave_inner=True, metric_format='{value:0.3f}')\n        ]).history\n    steps += step\n    \n    # Collect history data\n    history['epochs'] = steps\n    history['ms'    ] = np.mean(score)\n    history['lr'    ] = get_lr(model)\n    print(history['epochs'],history['lr'],history['ms'])\n    histories.append(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f84b80385ff2adbac2528fe5642f55c5bb50ea29","trusted":true,"collapsed":true},"cell_type":"code","source":"model_name = 'mpiotte-standard'\nhistories  = []\nsteps      = 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2563e2e6b15e7ce23f94ad5a1093ad2e70861de4","trusted":true,"collapsed":true},"cell_type":"code","source":"if isfile('../input/humpback-whale-identification-model-files/mpiotte-standard.model'):\n    tmp = keras.models.load_model('../input/humpback-whale-identification-model-files/mpiotte-standard.model')\n    model.set_weights(tmp.get_weights())\nelse:\n    # epoch -> 10\n    make_steps(10, 1000)\n    ampl = 100.0\n    for _ in range(10):\n        print('noise ampl.  = ', ampl)\n        make_steps(5, ampl)\n        ampl = max(1.0, 100**-0.1*ampl)\n    # epoch -> 150\n    for _ in range(18): make_steps(5, 1.0)\n    # epoch -> 200\n    set_lr(model, 16e-5)\n    for _ in range(10): make_steps(5, 0.5)\n    # epoch -> 240\n    set_lr(model, 4e-5)\n    for _ in range(8): make_steps(5, 0.25)\n    # epoch -> 250\n    set_lr(model, 1e-5)\n    for _ in range(2): make_steps(5, 0.25)\n    # epoch -> 300\n    weights = model.get_weights()\n    model, branch_model, head_model = build_model(64e-5,0.0002)\n    model.set_weights(weights)\n    for _ in range(10): make_steps(5, 1.0)\n    # epoch -> 350\n    set_lr(model, 16e-5)\n    for _ in range(10): make_steps(5, 0.5)    \n    # epoch -> 390\n    set_lr(model, 4e-5)\n    for _ in range(8): make_steps(5, 0.25)\n    # epoch -> 400\n    set_lr(model, 1e-5)\n    for _ in range(2): make_steps(5, 0.25)\n    model.save('mpiotte-standard.model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a02a1fa6e97259186174afc82a4599e03d047d2b"},"cell_type":"markdown","source":"#  Generating the submission file\nThe basic strategy is this. For each picture from the test set:\n\n1. If the image duplicates one or more training set images, add the whales (possibly more than one) from the training image as the top candidates.\n1. For each image not new_whale from the training set, compute the image score, which is the model score for the image pair.\n1. For each whale from the training set, compute the whale score as the maximum image score for this whale.\n1. Add new_whale with a fixed new whale score of 'threshold'.\n1. Sort the whales in decreasing score.\n\nDuplicate image are free answers, assuming there is no tagging error. For new_whale, the algorithm will chose high confidence prediction first, then insert new_whale, then low confidence predictions. 'threshold' is selected through trial and error, although most model variants perform best with a value of 'threshold' that results in ~7100 test images with new_whale as first choice, something that can be measured without submitting a prediction to Kaggle."},{"metadata":{"_uuid":"74332a27926d3bf1b72f7b0d027baf6da053a52b","trusted":true,"collapsed":true},"cell_type":"code","source":"# Not computing the submission in this notebook because it is a little slow. It takes about 15 minutes on setup with a GTX 1080.\nimport gzip\n\ndef prepare_submission(threshold, filename):\n    \"\"\"\n    Generate a Kaggle submission file.\n    @param threshold the score given to 'new_whale'\n    @param filename the submission file name\n    \"\"\"\n    vtop  = 0\n    vhigh = 0\n    pos   = [0,0,0,0,0,0]\n    with gzip.open(filename, 'wt', newline='\\n') as f:\n        f.write('Image,Id\\n')\n        for i,p in enumerate(tqdm_notebook(submit)):\n            t = []\n            s = set()\n            a = score[i,:]\n            for j in list(reversed(np.argsort(a))):\n                h = known[j]\n                if a[j] < threshold and new_whale not in s:\n                    pos[len(t)] += 1\n                    s.add(new_whale)\n                    t.append(new_whale)\n                    if len(t) == 5: break;\n                for w in h2ws[h]:\n                    assert w != new_whale\n                    if w not in s:\n                        if a[j] > 1.0:\n                            vtop += 1\n                        elif a[j] >= threshold:\n                            vhigh += 1\n                        s.add(w)\n                        t.append(w)\n                        if len(t) == 5: break;\n                if len(t) == 5: break;\n            if new_whale not in s: pos[5] += 1\n            assert len(t) == 5 and len(s) == 5\n            f.write(p + ',' + ' '.join(t[:5]) + '\\n')\n    return vtop,vhigh,pos\n\nif False:\n    # Find elements from training sets not 'new_whale'\n    h2ws = {}\n    for p,w in tagged.items():\n        if w != new_whale: # Use only identified whales\n            h = p2h[p]\n            if h not in h2ws: h2ws[h] = []\n            if w not in h2ws[h]: h2ws[h].append(w)\n    known = sorted(list(h2ws.keys()))\n\n    # Dictionary of picture indices\n    h2i   = {}\n    for i,h in enumerate(known): h2i[h] = i\n\n    # Evaluate the model.\n    fknown  = branch_model.predict_generator(FeatureGen(known), max_queue_size=20, workers=10, verbose=0)\n    fsubmit = branch_model.predict_generator(FeatureGen(submit), max_queue_size=20, workers=10, verbose=0)\n    score   = head_model.predict_generator(ScoreGen(fknown, fsubmit), max_queue_size=20, workers=10, verbose=0)\n    score   = score_reshape(score, fknown, fsubmit)\n\n    # Generate the subsmission file.\n    prepare_submission(0.99, 'mpiotte-standard.csv.gz')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb8f53368668723f327040bb3b058f005d38ba77"},"cell_type":"markdown","source":"# Bootstrapping and ensemble\nThe mpiotte-standard.model is good for a 0.766 score.\n\nBecause the training data set is small, and the test set is larger, bootstrapping is a good candidate to improve the score. In this context, bootstrapping means using the model to automatically generate additional training example, and retrained the model over this larger dataset. For this experiment, I selected all images from the test set for which the mpiotte-standard model provides a single whale prediction with a score > 0.999999 (score is just a number use to rank whales, it is **not** a probability).  "},{"metadata":{"_uuid":"b832dd2d0d837eea499b129184f6633e193ade48"},"cell_type":"markdown","source":"## Bootstrapping"},{"metadata":{"_uuid":"fcef7ef0b197305f6944e99346451cd3bca63fce","trusted":true},"cell_type":"code","source":"with open('../input/humpback-whale-identification-model-files/bootstrap.pickle', 'rb') as f:\n    bootstrap = pickle.load(f)\nlen(bootstrap), list(bootstrap.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42062beebd3413eeced9706ddf629406dba773f3"},"cell_type":"markdown","source":"Submitting these 1885 pictures as a submission show that this set if over 93% accurate.\n\nAdding these files to the training set and re-running the training from scratch generates the mpiotte.bootstrap.model, also included in the dataset. This model as a slightly better score of 0.744 with a threshold=0.989."},{"metadata":{"_uuid":"664531df0a9fd9cf7f59d02b47cf0a612a3d9522"},"cell_type":"markdown","source":"## Ensemble"},{"metadata":{"_uuid":"f294a99f12859710df82c9c6f663d5d0d7f3aaa9"},"cell_type":"markdown","source":"The best score is obtained by an ensemble of the mpiotte-standard.model and mpiotte-bootstrap.model. Both of these models make different errors because of their nature, which make them good candidate for ensembling:\n\n* The standard model is trained on the smallest training set, and thus has more potential for overfitting.\n* The bootstrap model is trained on more data, however the tagging accuracy is lower since the bootstrap data is only 93% accurate.\n\nThe assembly strategy consist in compute a score matrix (or dimension test by train) that is a linear combination of the standard and bootstrap model. Generation of the submission using the score matrix is unchanged. Trial and error suggest a weight of 0.45 for the standard model and 0.55 for the bootstrap model.\n\nThe resulting ensemble as an accuracy of **0.78563** using a threshold of 0.92. It is interesting to note how the 'threshold' value for the ensemble is much lower, which is consistent with the fact that both models make different errors, and thus the ensemble scores are typically lower than the individual models, which are grossly optimistic about they guesses."},{"metadata":{"_uuid":"69020ecc39f36ad16c670da837e6c336580525e8"},"cell_type":"markdown","source":"# Visualization\nThis section explores the model through some visualizations."},{"metadata":{"_uuid":"ca8713813b6e8d8fff5ce8c031f4d2cd05f48561"},"cell_type":"markdown","source":"## Feature weights"},{"metadata":{"_uuid":"3ec585afe83f6fa115c2867c8ff60f0f3f066b0a"},"cell_type":"markdown","source":"As was discussed in the model description, the head model makes a weighted sum of the features, allowing for negative weights. We can verify that we see a combination of positive and negative weights, that confirm that some features, when matched, reduce the probability of matching whales. This could be that we match uniform, unicolor flukes, which is less likely to be correct that mathing flukes with multiple caracteristic markings."},{"metadata":{"_uuid":"a67b17eb5b0472789036dff08b064cb709d13389","trusted":true},"cell_type":"code","source":"w = head_model.layers[-1].get_weights()[0]\nw = w.flatten().tolist()\nw = sorted(w)\nfig, axes = plt.subplots(1,1)\naxes.bar(range(len(w)), w)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dee1a76306cba30215867133189f58f56a6c90e"},"cell_type":"markdown","source":"We can also check how 'per feature' network behave for different feature values.\n\nWhat we expect to see is that equal zero feature should produce a smaller output than similar large values. At the same time, very dissimilar values must be penalized."},{"metadata":{"_uuid":"2af9ec4e2c5df9faa21aba4ed37833486ade6b55","trusted":true,"collapsed":true},"cell_type":"code","source":"# Construct the head model with linear activation\n_, _, tmp_model = build_model(64e-5,0, activation='linear')\ntmp_model.set_weights(head_model.get_weights())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e937b7a76cc7e2b6e7474592a993e9d837437c03","trusted":true},"cell_type":"code","source":"# Evaluate the model for constant vectors.\na = np.ones((21*21,512),dtype=K.floatx())\nb = np.ones((21*21,512),dtype=K.floatx())\nfor i in range(21):\n    for j in range(21):\n        a[21*i + j] *= float(i)/10.0\n        b[21*i + j] *= float(j)/10.0\nx    = np.arange(0.0, 2.01, 0.1, dtype=K.floatx())\nx, y = np.meshgrid(x, x)\nz    = tmp_model.predict([a,b], verbose=0).reshape((21,21))\nx.shape, y.shape, z.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0010e75e700bfa3501d7ff8bf73c7be764ad62e0"},"cell_type":"markdown","source":"## Pseudo-distance function"},{"metadata":{"_uuid":"ab38b8c09129211f8d79cfd569ba4740cb56e540","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(x, y, z, cmap=cm.coolwarm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ddf0cfc7f773b64bf7c2ba1fc490b10dc74e793"},"cell_type":"markdown","source":"Not very easy to see, but still the largest output (best whale match) occurs for matching features with large values. Matching zeros get llower value.\n\nJust the colormap is probably easier to see. This confirms are initial assumptions."},{"metadata":{"_uuid":"1b76640bfb5f4c466a8077d38fc928246c8949df","trusted":true},"cell_type":"code","source":"from matplotlib.colors import BoundaryNorm\nfrom matplotlib.ticker import MaxNLocator\n\nlevels = MaxNLocator(nbins=15).tick_values(z.min(), z.max())\nfig = plt.figure()\nax = fig.add_subplot(111)\ncf = ax.contourf(x, y, z, levels=levels, cmap=cm.coolwarm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef170b3c74a41e52c64750b807d3ca56ecdda144"},"cell_type":"markdown","source":"## Feature activation\nThis section attempts to reconstruct image that maximally activate a feature. This provides some insight on the filtering process.\n\nThe code to generate images is modified from examples found in [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by Francois Chollet."},{"metadata":{"_uuid":"ced4199e3979bc63c12949047ca88af4fb9c1538","trusted":true},"cell_type":"code","source":"from scipy.ndimage import gaussian_filter\n\ndef show_filter(filter, blur):\n    np.random.seed(1)\n    noise   = 0.1 # Initial noise\n    step    = 1 # Gradient step\n    \n    # Construct the function\n    inp     = branch_model.layers[0].get_input_at(0)\n    loss    = K.mean(branch_model.layers[-3].output[0,2:4,2:4,filter]) # Stimulate the 4 central cells\n    grads   = K.gradients(loss, inp)[0]\n    grads  /= K.sqrt(K.mean(K.square(grads))) + K.epsilon()\n    iterate = K.function([inp],[grads])\n    img     = (np.random.random(img_shape) -0.5)*noise\n    img     = np.expand_dims(img, 0)\n\n    # Use gradient descent to form image\n    for i in range(200):\n        grads_value = iterate([img])[0]\n        # Blurring a little creates nicer images by reducing reconstruction noise\n        img = gaussian_filter(img + grads_value*step, sigma=blur)\n\n    # Clip the image to improve contrast\n    avg  = np.mean(img)\n    std  = sqrt(np.mean((img - avg)**2))\n    low  = avg - 5*std\n    high = avg + 5*std\n    return array_to_img(np.minimum(high, np.maximum(low, img))[0])\n\n# Show the first 25 features (of 512)\nshow_whale([show_filter(i, 0.5) for i in tqdm_notebook(range(25))], per_row=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38ac924d4edbedd64d312c99e353828e409e8ecb","collapsed":true},"cell_type":"markdown","source":"# Off topic"},{"metadata":{"_uuid":"596d0a17807e10828730542ea6fd1e90a42f60af"},"cell_type":"markdown","source":"## Training Scalability\nAs described, training the base model takes about 2 days, and the bootstrap version a little under 3 days, using a i7-8700 CPU and GTX 1080 GPU. More than 50% of the time is spent solving the Linear Assignment Problem because the algorithm used has complexity \\\\( O(n^3) \\\\) and provides an exact solution. However, the score matrix is randomized, so investing a lot of time to compute an exact solution to a randomized input is wasteful. In the context of this competition, with no constraint on runtime and a small dataset, it was a pragmatic choice. However, to scale this approach, a less costly randomized matching heuristic would be more effective.\n\nAnother approach to training scalability would be to partition the training data into different subset, each subset being processed separately to match image pairs. The subset can be reconstructed randomly each time the cost matrix is computed. This would be effective not only for the Linear Assignment Problem part, but also when computing the cost matrix, which still has complexity \\\\( O(n^2) \\\\). By fixing the subset size to a reasonable value, the complexity then grows linearly with the number of subsets, allowing larger training datasets..  "},{"metadata":{"_uuid":"ac0cbe07b888d80c4ffb7121bfd9a4f658816b65"},"cell_type":"markdown","source":"## Interesting results and scores\nScore | Description\n---:|-------\n0.786 | best score obtained by a linear combination of the standard model with the bootstrap model\n0.774 | bootstrapped model\n0.766 | standard model\n0.752 | VGG like CNN, trained like standard model\n0.728 | standard model without L2 regularization (result after 250 epochs)\n0.714 | standard model without exclusion list, rotation list and bounding box model (i.e. no manual judgement on the training set)\n0.423 | duplicate images and new_whale submission\n0.325 | new_whale submission\n0.107 | duplicate images only\n"},{"metadata":{"_uuid":"1ad0edb6a1fca52effac15d40addd8560e854f28"},"cell_type":"markdown","source":"## Validation set\nI have not discussed validation data until now. During development, I used a validation set composed of 570 images from the training set to test ideas and to tune the training procedure. However, greater accuracy can be achieved by retraining the model using all the data,  repeating he procedure that was successful on the validation set. This notebook essentially describes this final retraining, and thus there is no validation set involved."},{"metadata":{"_uuid":"4b8ee534a637334e163acb2031cce28475a71d7c","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}