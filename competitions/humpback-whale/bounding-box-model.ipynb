{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "source": [
    "# Bounding Box Model\n",
    "This notebook explains how to generage a bounding box model.<br/>\n",
    "While many of the whale pictures in the dataset are already cropped tight around the whale fluke, in some images the whale fluke occupies only a small area of the picture. Zooming in the relevant part of the picture provides greater accuracy to a classification model. To automate the process, this notebook explains how to construct a convolutional neural network (CNN) capable of estimating the whale bounding box.<br/>\n",
    "Using this model, whale pictures can be cropped automatically to a more uniform appearance. This facilitates training of classification models, and improves the test accuracy.<br/>\n",
    "Training of the bounding box model is performed over a dataset of 1200 bounding boxes for pictures selected from the Humpback Whale Identification Challenge training set. 1000 pictures are used for training, while 200 are reserved for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dce51ea560d9b30bd2dc0c72ae97804f006f6a60",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Suppress annoying stderr output when importing keras.\n",
    "import sys\n",
    "old_stderr = sys.stderr\n",
    "sys.stderr = open('/dev/null', 'w')\n",
    "import keras\n",
    "sys.stderr = old_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Read the cropping dataset\n",
    "Once decoded, the variable *data* is a list of tuples. Each tuple contains the picture filename and a list of coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41d55d57aa4404de5e00e4170b4d2b81511ebfae"
   },
   "outputs": [],
   "source": [
    "with open('../input/humpback-whale-identification-fluke-location/cropping.txt', 'rt') as f: data = f.read().split('\\n')[:-1]\n",
    "data = [line.split(',') for line in data]\n",
    "data = [(p,[(int(coord[i]),int(coord[i+1])) for i in range(0,len(coord),2)]) for p,*coord in data]\n",
    "data[0] # Show an example: (picture-name, [coordinates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b094e4935bfa0ff788a588554508769911e6de29"
   },
   "source": [
    "The coordinates represent points on the fluke edge. The extremum values can be used to construct a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf0668be8370405a366864491b40043429e96b71"
   },
   "outputs": [],
   "source": [
    "from PIL import Image as pil_image\n",
    "from PIL.ImageDraw import Draw\n",
    "from os.path import isfile\n",
    "\n",
    "def expand_path(p):\n",
    "    if isfile('../input/whale-categorization-playground/train/' + p): return '../input/whale-categorization-playground/train/' + p\n",
    "    if isfile('../input/whale-categorization-playground/test/' + p): return '../input/whale-categorization-playground/test/' + p\n",
    "    return p\n",
    "\n",
    "def read_raw_image(p):\n",
    "    return pil_image.open(expand_path(p))\n",
    "\n",
    "def draw_dot(draw, x, y):\n",
    "    draw.ellipse(((x-5,y-5),(x+5,y+5)), fill='red', outline='red')\n",
    "\n",
    "def draw_dots(draw, coordinates):\n",
    "    for x,y in coordinates: draw_dot(draw, x, y)\n",
    "\n",
    "def bounding_rectangle(list):\n",
    "    x0, y0 = list[0]\n",
    "    x1, y1 = x0, y0\n",
    "    for x,y in list[1:]:\n",
    "        x0 = min(x0, x)\n",
    "        y0 = min(y0, y)\n",
    "        x1 = max(x1, x)\n",
    "        y1 = max(y1, y)\n",
    "    return x0,y0,x1,y1\n",
    "\n",
    "filename,coordinates = data[0]\n",
    "box = bounding_rectangle(coordinates)\n",
    "img = read_raw_image(filename)\n",
    "draw = Draw(img)\n",
    "draw_dots(draw, coordinates)\n",
    "draw.rectangle(box, outline='red')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d558ea44539391ea789c845b774399792a90d8fb"
   },
   "source": [
    "# Image preprocessing code\n",
    "Images are preprocessed by:\n",
    "1. Converting to black&white;\n",
    "1. Compressing horizontally by a factor of 2.15 (the mean aspect ratio);\n",
    "1. Apply a random image transformation (only for training)\n",
    "1. Resizing to 128x128;\n",
    "1. Normalizing to zero mean and unit variance.\n",
    "\n",
    "These operation are performed by the following code that is later invoked when preparing the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d911ee27cdae1e2d0838119e621068a8cc24c4d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define useful constants\n",
    "img_shape  = (128,128,1)\n",
    "anisotropy = 2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbbf2f5525f59cca4d4d8214856180a1349319cc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage import affine_transform\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Read an image as black&white numpy array\n",
    "def read_array(p):\n",
    "    img = read_raw_image(p).convert('L')\n",
    "    return img_to_array(img)\n",
    "\n",
    "def build_transform(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    rotation        = np.deg2rad(rotation)\n",
    "    shear           = np.deg2rad(shear)\n",
    "    rotation_matrix = np.array([[np.cos(rotation), np.sin(rotation), 0], [-np.sin(rotation), np.cos(rotation), 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, height_shift], [0, 1, width_shift], [0, 0, 1]])\n",
    "    shear_matrix    = np.array([[1, np.sin(shear), 0], [0, np.cos(shear), 0], [0, 0, 1]])\n",
    "    zoom_matrix     = np.array([[1.0/height_zoom, 0, 0], [0, 1.0/width_zoom, 0], [0, 0, 1]])\n",
    "    shift_matrix    = np.array([[1, 0, -height_shift], [0, 1, -width_shift], [0, 0, 1]])\n",
    "    return np.dot(np.dot(rotation_matrix, shear_matrix), np.dot(zoom_matrix, shift_matrix))\n",
    "\n",
    "# Compute the coordinate transformation required to center the pictures, padding as required.\n",
    "def center_transform(affine, input_shape):\n",
    "    hi, wi = float(input_shape[0]), float(input_shape[1])\n",
    "    ho, wo = float(img_shape[0]), float(img_shape[1])\n",
    "    top, left, bottom, right = 0, 0, hi, wi\n",
    "    if wi/hi/anisotropy < wo/ho: # input image too narrow, extend width\n",
    "        w     = hi*wo/ho*anisotropy\n",
    "        left  = (wi-w)/2\n",
    "        right = left + w\n",
    "    else: # input image too wide, extend height\n",
    "        h      = wi*ho/wo/anisotropy\n",
    "        top    = (hi-h)/2\n",
    "        bottom = top + h\n",
    "    center_matrix   = np.array([[1, 0, -ho/2], [0, 1, -wo/2], [0, 0, 1]])\n",
    "    scale_matrix    = np.array([[(bottom - top)/ho, 0, 0], [0, (right - left)/wo, 0], [0, 0, 1]])\n",
    "    decenter_matrix = np.array([[1, 0, hi/2], [0, 1, wi/2], [0, 0, 1]])\n",
    "    return np.dot(np.dot(decenter_matrix, scale_matrix), np.dot(affine, center_matrix))\n",
    "\n",
    "# Apply an affine transformation to an image represented as a numpy array.\n",
    "def transform_img(x, affine):\n",
    "    matrix   = affine[:2,:2]\n",
    "    offset   = affine[:2,2]\n",
    "    x        = np.moveaxis(x, -1, 0)\n",
    "    channels = [affine_transform(channel, matrix, offset, output_shape=img_shape[:-1], order=1,\n",
    "                                 mode='constant', cval=np.average(channel)) for channel in x]\n",
    "    return np.moveaxis(np.stack(channels, axis=0), 0, -1)\n",
    "\n",
    "# Read an image for validation, i.e. without data augmentation.\n",
    "def read_for_validation(p):\n",
    "    x  = read_array(p)\n",
    "    t  = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    t  = center_transform(t, x.shape)\n",
    "    x  = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x,t \n",
    "\n",
    "# Read an image for training, i.e. including a random affine transformation\n",
    "def read_for_training(p):\n",
    "    x  = read_array(p)\n",
    "    t  = build_transform(\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(-5, 5),\n",
    "            random.uniform(0.9, 1.0),\n",
    "            random.uniform(0.9, 1.0),\n",
    "            random.uniform(-0.05*img_shape[0], 0.05*img_shape[0]),\n",
    "            random.uniform(-0.05*img_shape[1], 0.05*img_shape[1]))\n",
    "    t  = center_transform(t, x.shape)\n",
    "    x  = transform_img(x, t)\n",
    "    x -= np.mean(x, keepdims=True)\n",
    "    x /= np.std(x, keepdims=True) + K.epsilon()\n",
    "    return x,t   \n",
    "\n",
    "# Transform corrdinates according to the provided affine transformation\n",
    "def coord_transform(list, trans):\n",
    "    result = []\n",
    "    for x,y in list:\n",
    "        y,x,_ = trans.dot([y,x,1]).astype(np.int)\n",
    "        result.append((x,y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df8c54acaeb6bd4cba6dee323fe40b627142616c"
   },
   "source": [
    "# Prepare the corpus\n",
    "Split the corpus between training and validation data. Duplicate the training data 16 times to make reasonable size training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "639b3dbcc5174010b9de4971bc98c34d3a10ebeb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(data, test_size=200, random_state=1)\n",
    "train += train\n",
    "train += train\n",
    "train += train\n",
    "train += train\n",
    "len(train),len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "238f91cf49c438e766c4bf4bf7abf1e02da9d690"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from numpy.linalg import inv as mat_inv\n",
    "\n",
    "def show_whale(imgs, per_row=5):\n",
    "    n         = len(imgs)\n",
    "    rows      = (n + per_row - 1)//per_row\n",
    "    cols      = min(per_row, n)\n",
    "    fig, axes = plt.subplots(rows,cols, figsize=(24//per_row*cols,24//per_row*rows))\n",
    "    for ax in axes.flatten(): ax.axis('off')\n",
    "    for i,(img,ax) in enumerate(zip(imgs, axes.flatten())): ax.imshow(img.convert('RGB'))\n",
    "\n",
    "val_a = np.zeros((len(val),)+img_shape,dtype=K.floatx()) # Preprocess validation images \n",
    "val_b = np.zeros((len(val),4),dtype=K.floatx()) # Preprocess bounding boxes\n",
    "for i,(p,coords) in enumerate(tqdm_notebook(val)):\n",
    "    img,trans      = read_for_validation(p)\n",
    "    coords         = coord_transform(coords, mat_inv(trans))\n",
    "    x0,y0,x1,y1    = bounding_rectangle(coords)\n",
    "    val_a[i,:,:,:] = img\n",
    "    val_b[i,0]     = x0\n",
    "    val_b[i,1]     = y0\n",
    "    val_b[i,2]     = x1\n",
    "    val_b[i,3]     = y1\n",
    "\n",
    "idx  = 1\n",
    "img  = array_to_img(val_a[idx])\n",
    "img  = img.convert('RGB')\n",
    "draw = Draw(img)\n",
    "draw.rectangle(val_b[idx], outline='red')\n",
    "show_whale([read_raw_image(val[idx][0]), img], per_row=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13a67cf59860954a341a1a75a00a79eb52f8f45d",
    "collapsed": true
   },
   "source": [
    "The image on the left side is the original image. The image on the right side is converted to B&W, compressed horizontally, padded and resized to 128x128.<br.>\n",
    "The right side image is annotated with the transformed bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e84a7aa4f1bf8035f61f3be3a7da18ffb9e94ea6"
   },
   "source": [
    "The following class extends the Sequence class from keras to generate input image data augmentation on the fly. Each image is processed through a random affine transformation. The tagged boundary points are also transformed to adjust the dimension of the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50243ecf6d4f5b81fcf50a020b27c3a383bda8e0"
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class TrainingData(Sequence):\n",
    "    def __init__(self, batch_size=32):\n",
    "        super(TrainingData, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "    def __getitem__(self, index):\n",
    "        start = self.batch_size*index;\n",
    "        end   = min(len(train), start + self.batch_size)\n",
    "        size  = end - start\n",
    "        a     = np.zeros((size,) + img_shape, dtype=K.floatx())\n",
    "        b     = np.zeros((size,4), dtype=K.floatx())\n",
    "        for i,(p,coords) in enumerate(train[start:end]):\n",
    "            img,trans   = read_for_training(p)\n",
    "            coords      = coord_transform(coords, mat_inv(trans))\n",
    "            x0,y0,x1,y1 = bounding_rectangle(coords)\n",
    "            a[i,:,:,:]  = img\n",
    "            b[i,0]      = x0\n",
    "            b[i,1]      = y0\n",
    "            b[i,2]      = x1\n",
    "            b[i,3]      = y1\n",
    "        return a,b\n",
    "    def __len__(self):\n",
    "        return (len(train) + self.batch_size - 1)//self.batch_size\n",
    "\n",
    "random.seed(1)\n",
    "a, b = TrainingData(batch_size=5)[1]\n",
    "img  = array_to_img(a[0])\n",
    "img  = img.convert('RGB')\n",
    "draw = Draw(img)\n",
    "draw.rectangle(b[0], outline='red')\n",
    "show_whale([read_raw_image(train[0][0]), img], per_row=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6ecdd4b6ddaae3cefc4a02d82ce1a589d585b0f"
   },
   "source": [
    "The image on the left side is the original image. The image on the right side is converted to B&W, compressed horizontally, randomly transformed, padded and resized to 128x128.<br.>\n",
    "The right side image is annotated with the transformed bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a87ca0b18396d1860462120a84398fa072de356"
   },
   "source": [
    "# Keras Model\n",
    "The following code fragment shows the bounding box model construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90ca34d01e28107b862b389e23bd8df2a7907d35"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Input\n",
    "from keras.layers import BatchNormalization, Concatenate, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "def build_model(with_dropout=True):\n",
    "    kwargs     = {'activation':'relu', 'padding':'same'}\n",
    "    conv_drop  = 0.2\n",
    "    dense_drop = 0.5\n",
    "    inp        = Input(shape=img_shape)\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    x = Conv2D(64, (9, 9), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "\n",
    "    x = Conv2D(64, (2, 2), **kwargs, strides=2)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = Conv2D(64, (3, 3), **kwargs)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if with_dropout: x = Dropout(conv_drop, noise_shape=(None, 1, 1, int(x.shape[-1])))(x)\n",
    "\n",
    "    h = MaxPooling2D(pool_size=(1, int(x.shape[2])))(x)\n",
    "    h = Flatten()(h)\n",
    "    if with_dropout: h = Dropout(dense_drop)(h)\n",
    "    h = Dense(16, activation='relu')(h)\n",
    "\n",
    "    v = MaxPooling2D(pool_size=(int(x.shape[1]), 1))(x)\n",
    "    v = Flatten()(v)\n",
    "    if with_dropout: v = Dropout(dense_drop)(v)\n",
    "    v = Dense(16, activation='relu')(v)\n",
    "\n",
    "    x = Concatenate()([h,v])\n",
    "    if with_dropout: x = Dropout(0.5)(x)\n",
    "    x = Dense(4, activation='linear')(x)\n",
    "    return Model(inp,x)\n",
    "\n",
    "model = build_model(with_dropout=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d1ba2edcfb42d9f94291c99d9539e36243b52c0"
   },
   "source": [
    "Here are a few thoughts aboud the model:\n",
    "\n",
    " * The basic idea is mostly inspired from the VGG model, with a stack of 3x3 convolutions separated by pooling layers. Here max pooling is replaced by a 2x2 convolution with stride 2. It seemed more logical, as max pooling appears to lose some location information. In practice in makes little difference.\n",
    " * At the end, max pooling is used on rows and columns separately. For the fluke height, we don't care if it occurs on the left or right. Similarly for the width, we don't care if it occurs at the top or the bottom. Both sets are concatenated, but clearly one subset is aimed at finding left and right, and the other top and bottom.\n",
    " * A few changes from VGG include a larger kernel for the first convolution, batch normalization and dropout. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a0a7760683822fe319e43e922ba9d580271452d"
   },
   "source": [
    "The model is trained using data augmentation (random affine transformations) as generated by the TrainingData class defined above.<br/>\n",
    "Decreasing learning rate and early stopping is used when no significant progress is made.<br/>\n",
    "The model is trained three times. The weights are preserved between runs, but the learning rate is reset to the original value. This essentially cycles the learning rate between low and high values, which can be advantageous according to **Smith, L.N.**,  \"*Cyclical Learning Rates for Training Neural Networks*\", [arXiv:1506.01186](https://arxiv.org/abs/1506.01186)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a355b4c2f53422dfc30ac389000954d08a37736e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "for num in range(1, 4):\n",
    "    model_name = 'cropping-%01d.h5' % num\n",
    "    print(model_name)\n",
    "    model.compile(Adam(lr=0.032), loss='mean_squared_error')\n",
    "    model.fit_generator(\n",
    "        TrainingData(), epochs=50, max_queue_size=12, workers=4, verbose=1,\n",
    "        validation_data=(val_a, val_b),\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=9, min_delta=0.1, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', patience=3, min_delta=0.1, factor=0.25, min_lr=0.002, verbose=1),\n",
    "            ModelCheckpoint(model_name, save_best_only=True, save_weights_only=True),\n",
    "        ])\n",
    "    model.load_weights(model_name)\n",
    "    model.evaluate(val_a, val_b, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c93f5f3bceedc765f674ef197dbf3a312450d314",
    "collapsed": true
   },
   "source": [
    "Select the best of the three attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e89ffd1ace4057291242ea82b0c9f91dfbde8b09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('cropping-1.h5')\n",
    "loss1 = model.evaluate(val_a, val_b, verbose=0)\n",
    "model.load_weights('cropping-2.h5')\n",
    "loss2 = model.evaluate(val_a, val_b, verbose=0)\n",
    "model.load_weights('cropping-3.h5')\n",
    "loss3 = model.evaluate(val_a, val_b, verbose=0)\n",
    "model_name = 'cropping-1.h5'\n",
    "if loss2 <= loss1 and loss2 < loss3: model_name = 'cropping-2.h5'\n",
    "if loss3 <= loss1 and loss3 <= loss2: model_name = 'cropping-3.h5'\n",
    "model.load_weights(model_name)\n",
    "loss1, loss2, loss3, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ea09792a118595a6eef23e6d3e75029f539ff3a",
    "collapsed": true
   },
   "source": [
    "# Variance normalization\n",
    "Using batch normalization after dropout has a small problem. During training, dropout zeros some outputs, but scales up the remaining ones to maintain the output average. However, the output **variance** is not preserved. The variance is larger during training than it is during inference.<br/>\n",
    "Batch normalization also behaves differently during training and inference. During training batches are normalized, but at the same time a running average of batch mean and variance is computed. This running average is used as a sample estimate during inference. It should be immediately obvious that the running average value is not a good approximation of the sample variance during inference, because dropout behavior changes the variance.  See **Xiang Li, Shuo Chen, Xiaolin Hu, Jian Yang**, \"*Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift*\", [arXiv:1801.05134](https://arxiv.org/abs/1801.05134).<br/>\n",
    "One of the proposed solutions is to recompute the batch normalization running average without dropout, while freezing other layers. The resulting accuracy is expected to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2327ef9a1a9a60f7e7216880a48b9bcfa278e9e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = build_model(with_dropout=False)\n",
    "model2.load_weights(model_name)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28bfca6f2ea093672fd94209503c1f623f97936a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(Adam(lr=0.002), loss='mean_squared_error')\n",
    "model2.evaluate(val_a, val_b, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef74e0a191cf2811ceb8ae8c2c54f380692d0769",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recompute the mean and variance running average without dropout\n",
    "for layer in model2.layers:\n",
    "    if not isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = False\n",
    "model2.compile(Adam(lr=0.002), loss='mean_squared_error')\n",
    "model2.fit_generator(TrainingData(), epochs=1, max_queue_size=12, workers=6, verbose=1, validation_data=(val_a, val_b))\n",
    "for layer in model2.layers:\n",
    "    if not isinstance(layer, BatchNormalization):\n",
    "        layer.trainable = True\n",
    "model2.compile(Adam(lr=0.002), loss='mean_squared_error')\n",
    "model2.save('cropping.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9983ecd5228b4d7a1c66305894e4ba78541cfa9d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.evaluate(val_a, val_b, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5c6331910e492b55a8c642651e9d0332964c1063"
   },
   "source": [
    "# Explore the results\n",
    "The model is not trained over the validation set, so it represents a fair assessment of the bounding box model accuracy.<br/>\n",
    "The following figure shows the transformed whale images, the reference bounding boxes in red and the computed bounding boxes in yellow for all images from the bounding box validation set (200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b808f5da974a9f50a581be7ef6ba98b79e1b9d54",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for i,(p,coords) in enumerate(val[:25]):\n",
    "    a         = val_a[i:i+1]\n",
    "    rect1     = val_b[i]\n",
    "    rect2     = model2.predict(a).squeeze()\n",
    "    img       = array_to_img(a[0]).convert('RGB')\n",
    "    draw      = Draw(img)\n",
    "    draw.rectangle(rect1, outline='red')\n",
    "    draw.rectangle(rect2, outline='yellow')\n",
    "    images.append(img)\n",
    "show_whale(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bea4597d97cee09553b296f7f0e2866bfecbd149"
   },
   "source": [
    "# Generate best bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b39e595a5ebb5cdbf7b4b46e12d7573e2bf25e8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "tagged = [p for _,p,_ in read_csv('../input/whale-categorization-playground/train.csv').to_records()]\n",
    "submit = [p for _,p,_ in read_csv('../input/whale-categorization-playground/sample_submission.csv').to_records()]\n",
    "join = tagged + submit\n",
    "len(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b16b5f37e18482c177c166db2081ade9335994e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the picture is part of the bounding box dataset, use the golden value.\n",
    "p2bb = {}\n",
    "for i,(p,coords) in enumerate(data): p2bb[p] = bounding_rectangle(coords)\n",
    "len(p2bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d620288ef35cb74571b6425d247f81f8d81d8e22",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For other pictures, evaluate the model.\n",
    "p2bb = {}\n",
    "for p in tqdm_notebook(join):\n",
    "    if p not in p2bb:\n",
    "        img,trans         = read_for_validation(p)\n",
    "        a                 = np.expand_dims(img, axis=0)\n",
    "        x0, y0, x1, y1    = model2.predict(a).squeeze()\n",
    "        (u0, v0),(u1, v1) = coord_transform([(x0,y0),(x1,y1)], trans)\n",
    "        p2bb[p]           = (u0, v0, u1, v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbec7374070a47ba5019b10ea19ee9ff2c698a38",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bounding-box.pickle', 'wb') as f: pickle.dump(p2bb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ebfca659382623a38066784782f99a96c9f9171"
   },
   "source": [
    "# Show some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99abb236b371d3fe948c7765e1bb1317ffaca867",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "for p in tagged[:25]:\n",
    "    img         = read_raw_image(p).convert('RGB')\n",
    "    draw        = Draw(img)\n",
    "    x0,y0,x1,y1 = p2bb[p]\n",
    "    draw.line([(x0, y0),(x0,y1),(x1,y1),(x1,y0),(x0,y0)], fill='yellow', width=6)\n",
    "    samples.append(img)\n",
    "show_whale(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d971dd745d01e16626c6c8a984a0090ce64829b"
   },
   "source": [
    "# Generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "624b296aa0f31deaf7ae9fe57e6dbcbe3ead232d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.remove('cropping-1.h5')\n",
    "os.remove('cropping-2.h5')\n",
    "os.remove('cropping-3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3cdd795d335a94cd2f92ba5cbb0b3759e392276",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls *.pickle *.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "483bea830e61c5aafc8233cf3c26b17cd8f59dc5",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
